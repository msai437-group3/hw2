{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import gridspec\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You must use the Huggingface Emoji dataset for this project\n",
    "- You should divide this subset into training, validation and test sets using a 60/20/20 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1649 examples\n",
      "Test: 550 examples\n",
      "Validation: 550 examples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the \"emoji-dataset\"\n",
    "dataset = load_dataset(\"valhalla/emoji-dataset\")\n",
    "def split_dataset(dataset):\n",
    "    # Split the dataset into training, testing, and validation sets\n",
    "    train_ratio = 0.6\n",
    "    test_ratio = 0.4\n",
    "    validation_ratio = 0.5\n",
    "\n",
    "    # Use the train_test_split function to create the splits\n",
    "    splits = dataset[\"train\"].train_test_split(\n",
    "        test_size=test_ratio,\n",
    "        train_size=train_ratio,\n",
    "        shuffle=True,\n",
    "        # split_seed=42  # You can change this seed for reproducibility\n",
    "    )\n",
    "\n",
    "    # Access the resulting splits\n",
    "    train_dataset = splits[\"train\"]\n",
    "    test_dataset = splits[\"test\"]\n",
    "\n",
    "    # Further split the train_dataset for validation\n",
    "    train_splits = test_dataset.train_test_split(\n",
    "        test_size=validation_ratio,\n",
    "        train_size=1 - validation_ratio,\n",
    "        shuffle=True,\n",
    "        # split_seed=42\n",
    "    )\n",
    "\n",
    "    # Access the resulting splits\n",
    "    test_dataset = train_splits[\"train\"]\n",
    "    validation_dataset = train_splits[\"test\"]\n",
    "\n",
    "    # Print the number of examples in each split\n",
    "    print(f\"Train: {len(train_dataset)} examples\")\n",
    "    print(f\"Test: {len(test_dataset)} examples\")\n",
    "    print(f\"Validation: {len(validation_dataset)} examples\")\n",
    "    return train_dataset, test_dataset, validation_dataset\n",
    "\n",
    "train_dataset, test_dataset, validation_dataset = split_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You should use the image “text” to select a related subset of images to work with (e.g.,\n",
    "“face”, “superhero”, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify the text you want to match\n",
    "def filter_training(dataset, target_text) :\n",
    "\n",
    "    # Function to filter images based on text\n",
    "    def filter_by_text(example):\n",
    "        return target_text in example[\"text\"]\n",
    "\n",
    "    # Apply the filter function to get matching examples\n",
    "    matching_examples = train_dataset.filter(filter_by_text)\n",
    "    return matching_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, figsize=(15, 15), rows=10):\n",
    "    fig, axes = plt.subplots(rows, len(images) // rows, figsize=figsize)\n",
    "\n",
    "    if len(images) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, img in zip(axes.flatten(), images):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        # ax.set_title(title)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "target_text = \"flag\"  # Replace with your specific text\n",
    "# Display the matching images\n",
    "images = filter_training(train_dataset, target_text)['image']\n",
    "# show_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You are encouraged to reduce the resolution of the dataset to something appropriate for\n",
    "computational resources (e.g., 64 x 64 x 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reshape_dataset(images):\n",
    "    image_list = []\n",
    "    for image in images:\n",
    "        # image = e\n",
    "        # Resize the image to 64x64 pixels\n",
    "        resized_image = image.resize((64, 64))\n",
    "        # Convert the image to a NumPy array\n",
    "        image_array = np.array(resized_image)\n",
    "        image_list.append(image_array)\n",
    "    # Convert the list of arrays to a single NumPy array\n",
    "    images = np.array(image_list)\n",
    "    # print(f\"Image size: {images.shape}\")\n",
    "    # print(f\"Number of images: {len(image_list)}\")\n",
    "    return images\n",
    "\n",
    "# Assuming train_dataset is your dataset\n",
    "images = filter_training(train_dataset, target_text)['image']\n",
    "len_train_images = len(images)\n",
    "reshaped_images = reshape_dataset(images)\n",
    "\n",
    "# Check if the length matches\n",
    "# print(f\"Length of train_dataset: {len_train_images}\")\n",
    "\n",
    "# Batch and shuffle the data using DataLoader\n",
    "# batch_size = BATCH_SIZE\n",
    "# shuffle = True  # Set to True if you want to shuffle the data\n",
    "# train_loader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing the Generating Neural Network\n",
    "\n",
    "- fully connected layer that takes an input from noise_dim and outputs to a size of 1024.\n",
    "- A ReLU activation function.\n",
    "- Another fully connected layer, this time maintaining a size of 1024.\n",
    "- nother ReLU activation function.\n",
    "- A final fully connected layer with a size of 784.\n",
    "- A TanH activation function, which is used to ensure the output image values are within the range of -1 to 1.\n",
    "\n",
    "From the Paper:\n",
    "Architecture guidelines for stable Deep Convolutional GANs\n",
    "- Replace any pooling layers with strided convolutions (discriminator) and fractional-strided\n",
    "convolutions (generator).\n",
    "- Use batchnorm in both the generator and the discriminator.\n",
    "- Remove fully connected hidden layers for deeper architectures.\n",
    "- Use ReLU activation in generator for all layers except for the output, which uses Tanh.\n",
    "- Use LeakyReLU activation in the discriminator for all layers|\n",
    "- Item 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=100, out_features=12544, bias=False)\n",
      "  (1): BatchNorm1d(12544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "  (3): Unflatten(dim=1, unflattened_size=(256, 7, 7))\n",
      "  (4): ConvTranspose2d(256, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "  (7): ConvTranspose2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (8): Tanh()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def generator_model(input_size = 100, hidden_size =7*7*256):\n",
    "    layer1 = [\n",
    "        nn.Linear(in_features=input_size, out_features=hidden_size, bias=False),\n",
    "        nn.BatchNorm1d(hidden_size),\n",
    "        # allows the gradients to flow better through the model \n",
    "        nn.LeakyReLU(inplace=True)\n",
    "    ]\n",
    "    layer2 = [nn.Unflatten(1, (256, 7, 7))]\n",
    "\n",
    "    layer3 = [\n",
    "        nn.ConvTranspose2d(256, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(inplace=True)\n",
    "            \n",
    "    ]\n",
    "    layer4 = [\n",
    "        nn.ConvTranspose2d(128, 1, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
    "        nn.Tanh()\n",
    "    ]\n",
    "    layers = layer1 + layer2 + layer3 + layer4\n",
    "    model = nn.Sequential(*layers)\n",
    "    return model\n",
    "# Create an instance of the generator\n",
    "generator = generator_model()\n",
    "# print(generator)\n",
    "# noise = torch.randn(images.size(0), noise_dim)\n",
    "\n",
    "#     # Generator forward pass\n",
    "# generated_images = generator(noise)\n",
    "# noise = torch.randn(1, 100) \n",
    "# generated_image = generator(noise)\n",
    "# print(generated_image)\n",
    "# plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "  (2): Dropout(p=0.3, inplace=False)\n",
      "  (3): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (4): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "  (5): Dropout(p=0.3, inplace=False)\n",
      "  (6): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (7): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "  (8): Dropout(p=0.3, inplace=False)\n",
      "  (9): Flatten(start_dim=1, end_dim=-1)\n",
      "  (10): Linear(in_features=6272, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def discriminator_model():\n",
    "    layer1 = [\n",
    "        nn.Conv2d(1, 64, kernel_size= 5, stride= 2, padding= 2),\n",
    "        nn.LeakyReLU(inplace=True),\n",
    "        nn.Dropout(0.3)\n",
    "    ]\n",
    "    layer2 = [\n",
    "        nn.Conv2d(64, 128, kernel_size=5, stride= 2, padding=2),\n",
    "        nn.LeakyReLU(inplace=True),\n",
    "        nn.Dropout(0.3)\n",
    "            \n",
    "    ]\n",
    "    layer3 = [\n",
    "        nn.Conv2d(128, 256, kernel_size=5, stride= 2, padding=2),\n",
    "        nn.LeakyReLU(inplace=True),\n",
    "        nn.Dropout(0.3)\n",
    "    ]\n",
    "    layers = layer1 + layer2 + layer3 + [nn.Flatten(), nn.Linear(6272, 1)]\n",
    "\n",
    "    model = nn.Sequential(*layers)\n",
    "    return model\n",
    "discriminator = discriminator_model()\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    target = torch.ones_like(fake_output)\n",
    "    F.mse_loss(fake_output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    target_real = torch.ones_like(real_output)\n",
    "    target_fake = torch.zeros_like(fake_output)\n",
    "\n",
    "    real_loss = F.mse_loss(real_output, target_real)\n",
    "    fake_loss = F.mse_loss(fake_output, target_fake)\n",
    "\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming images is a PyTorch tensor\n",
    "\n",
    "EPOCHS = 5\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "seed = torch.randn(num_examples_to_generate, noise_dim)\n",
    "\n",
    "# Assuming you already have generator, discriminator, generator_loss, discriminator_loss defined\n",
    "\n",
    "# Initialize optimizers\n",
    "generator_optimizer = optim.Adam(generator.parameters(), lr=0.001)\n",
    "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=0.001)\n",
    "def train_step(images):\n",
    "    noise = torch.randn(images.size(0), noise_dim)\n",
    "\n",
    "    # Generator forward pass\n",
    "    generated_images = generator(noise)\n",
    "    # Discriminator forward pass for real and fake images\n",
    "    real_output = discriminator(images)\n",
    "    fake_output = discriminator(generated_images.detach())\n",
    "\n",
    "    # Calculate losses\n",
    "    # gen_loss = generator_loss(fake_output)\n",
    "    # disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    # Backpropagation\n",
    "    # generator_optimizer.zero_grad()\n",
    "    # discriminator_optimizer.zero_grad()\n",
    "\n",
    "    # gen_loss.backward(retain_graph=True)\n",
    "    # disc_loss.backward()\n",
    "\n",
    "    # Update weights\n",
    "    # generator_optimizer.step()\n",
    "    # discriminator_optimizer.step()\n",
    "    return generated_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x2048 and 6272x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Convert the list of transformed images to a torch tensor\u001b[39;00m\n\u001b[1;32m     14\u001b[0m transformed_images \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(transformed_images)\n\u001b[0;32m---> 15\u001b[0m trained_images \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m generated_unflattened \u001b[38;5;241m=\u001b[39m trained_images\u001b[38;5;241m.\u001b[39msqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m generate_images \u001b[38;5;241m=\u001b[39m generated_unflattened\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[0;32mIn[83], line 19\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m     17\u001b[0m generated_images \u001b[38;5;241m=\u001b[39m generator(noise)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Discriminator forward pass for real and fake images\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m real_output \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m fake_output \u001b[38;5;241m=\u001b[39m discriminator(generated_images\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Calculate losses\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# gen_loss = generator_loss(fake_output)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# disc_loss = discriminator_loss(real_output, fake_output)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# generator_optimizer.step()\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# discriminator_optimizer.step()\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x2048 and 6272x1)"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    # filter_training(dataset, target_text)\n",
    "    # Display the matching images\n",
    "    target_text = \"flag\"  # Replace with your specific text\n",
    "    # show_images(filter_training(train_dataset, target_text)['image'])\n",
    "    filtered_dataset = filter_training(train_dataset, target_text)['image']\n",
    "    images = reshape_dataset(filtered_dataset)\n",
    "    for batch_images in images:  # Assuming each batch is a tuple of (images, labels)\n",
    "        #transforms.ToTensor: Convert a PIL image or numpy.ndarray to tensor.\n",
    "        transform = transforms.ToTensor()\n",
    "        # Apply the transform to each image in the batch\n",
    "        transformed_images = [transform(img) for img in batch_images]\n",
    "        # Convert the list of transformed images to a torch tensor\n",
    "        transformed_images = torch.stack(transformed_images)\n",
    "        trained_images = train_step(transformed_images)\n",
    "        generated_unflattened = trained_images.squeeze(dim=1)\n",
    "        generate_images = generated_unflattened.detach().numpy()\n",
    "show_images(generate_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RuntimeError: mat1 and mat2 shapes cannot be multiplied (64x2304 and 4096x1)\n",
    "# RuntimeError: mat1 and mat2 shapes cannot be multiplied (64x2048 and 4096x1)\n",
    "# RuntimeError: mat1 and mat2 shapes cannot be multiplied (64x2048 and 6272x1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
