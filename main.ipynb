{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import io\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Before running this script\n",
    "1. Please download below grid search files, and put them in the same folder as this notebook file:\n",
    "https://github.com/msai437-group3/hw2/blob/mikky/classification_params_grid_search.csv\n",
    "https://github.com/msai437-group3/hw2/blob/mikky/autoencoder_params_grid_search.csv\n",
    "2. Please define the path to your dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30596a7240d5ecba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATA_PATH = '/Users/mikky/Downloads/train-00000-of-00001-38cc4fa96c139e86.parquet'\n",
    "emojis = pd.read_parquet(DATA_PATH, engine='pyarrow')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e3ec31bb8352ad4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image = Image.open(io.BytesIO(emojis.iloc[0]['image']['bytes']))\n",
    "image.size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59f17959a2add9d5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.imshow(image)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5bced0030636ef3a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 1\n",
    "(5.0points) Implement and train your autoencoder on the subset of the Emoji dataset that you selected and augmented:\n",
    "a. describe your dataset and the steps that you used to create it,\n",
    "b. provide a summary of your architecture(see Adversarial Examples Notebook)\n",
    "c. discuss and explain your design choices,\n",
    "d. list hyper-parameters used in the model,\n",
    "e. plot learning curves for training and validation loss as a function of training epochs,\n",
    "f. provide the final average error of your autoencoder on your test set, and\n",
    "g. discuss any decisions or observations that you find relevant."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "adf60cdd6a61918"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_words = emojis['text'].str.split(' ').explode()\n",
    "word_counts = Counter(all_words)\n",
    "word_counts_df = pd.DataFrame(word_counts.items(), columns=['Word', 'Count'])\n",
    "# Rank the words by count from big to small\n",
    "word_counts_df = word_counts_df.sort_values('Count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "word_counts_df[:20]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6bcf5a4d98334258"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We only look at top 20 words with the most word frequency to ensure enough data for training. \n",
    "Out of 20, we will choose entity word such as man, woman, flag, face, male, female, hand, person. \n",
    "Let's take a look at these words and their correspondent pictures."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52fdbf308caff950"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "candicate_list = ['man', 'woman', 'flag', 'face', 'male', 'female', 'hand', 'person']\n",
    "\n",
    "def format_text(text, max_words=4):\n",
    "    # format the text, max_words: maximum number of words in one line\n",
    "    words = text.split()\n",
    "    lines = [' '.join(words[i:i + max_words]) for i in range(0, len(words), max_words)]\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "def show_emojis(filtered_df, fig_size=(25,25), subplot=(10,10), num=100, maximum_word=4):\n",
    "    # show emoji pictures\n",
    "    # print (filtered_df['text'][0:20])\n",
    "    resized_images = [{index:Image.open(io.BytesIO(row['image']['bytes'])).resize((128, 128))} for index, row in filtered_df.iterrows()][:num]\n",
    "    \n",
    "    plt.figure(figsize=fig_size)\n",
    "    for i, img_dict in enumerate(resized_images, start=1):\n",
    "        ax = plt.subplot(subplot[0], subplot[1], i)\n",
    "        ax.axis('off')\n",
    "        plt.imshow(np.array(list(img_dict.values())[0]))\n",
    "        ax.text(0.5, -0.5, format_text(filtered_df['text'][list(img_dict.keys())[0]], max_words=maximum_word), fontsize=12, ha='center', transform=ax.transAxes)\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.1, hspace=1.2)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79fedf622ebeec52"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's look at the example pictures of each word:\n",
    "\n",
    "### 1. man"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e80bb196cb06d11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filtered_df = emojis[emojis['text'].apply(lambda x: 'man' in x.split())]\n",
    "show_emojis(filtered_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2fb009119aec315"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. woman"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4262366818f9bb3f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filtered_df = emojis[emojis['text'].apply(lambda x: 'woman' in x.split())]\n",
    "show_emojis(filtered_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e83e11f0c6e99c6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. flag"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f811205bdb7f78"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filtered_df = emojis[emojis['text'].apply(lambda x: 'flag' in x.split())]\n",
    "show_emojis(filtered_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fc25fed5c673f6e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. face"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "109130918d4916ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filtered_df = emojis[emojis['text'].apply(lambda x: 'face' in x.split())]\n",
    "show_emojis(filtered_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44466a8f6828d2d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. male"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "606997562aa41767"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filtered_df = emojis[emojis['text'].apply(lambda x: 'male' in x.split())]\n",
    "show_emojis(filtered_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2e65c5d56984a3f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6. female"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4403a82da66079c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filtered_df = emojis[emojis['text'].apply(lambda x: 'female' in x.split())]\n",
    "show_emojis(filtered_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e583c6eeb3c519df"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7. hand"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b07133f52f4a3cf3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filtered_df = emojis[emojis['text'].apply(lambda x: 'hand' in x.split())]\n",
    "show_emojis(filtered_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "569664b8e352b2fc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 8. person"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd0b540b66ec4804"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filtered_df = emojis[emojis['text'].apply(lambda x: 'person' in x.split())]\n",
    "show_emojis(filtered_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "958e6af4783ef986"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "Candidates: ['man', 'woman', 'flag', 'face', 'male', 'female', 'hand', 'person']\n",
    "</br>\n",
    "\n",
    "Choosing Criteria:\n",
    "1. The data share common characteristics.\n",
    "2. For question 2, the data should have clear classfications.\n",
    "3. For question 3, the data should be easy to blend with each other.\n",
    "</br>\n",
    "\n",
    "Based on above criterias, we choose below candidates:\n",
    "man, woman, face, male, female, person\n",
    "</br>\n",
    "\n",
    "Reasons why we do not choose hand and flag:\n",
    "    - flags are too similar, which is not suitable for question 2 classification.\n",
    "    - hands will become horrible if different hand gestures are blended."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53511e3e7077f79e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We pick face.\n",
    "\n",
    "let's take a look at the full face dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85264df529f3a8f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "face_df = emojis[emojis['text'].apply(lambda x: 'face' in x.split())]\n",
    "show_emojis(face_df, fig_size=(30,40), subplot=(12,16), num=len(face_df), maximum_word=2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "838f6e6909dad631"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since \"clock faces\" are totally different from other faces, they just share same word of \"face\", so we will remove clock faces from the dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f306b7bab00c050"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "face_df = emojis[emojis['text'].apply(lambda x: 'face' in x.split() and 'clock' not in x.split())].copy()\n",
    "len(face_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65fe7b0458c33656"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "face_texts = face_df['text']\n",
    "face_imgs = np.array([np.array(Image.open(io.BytesIO(row['image']['bytes']))) for index, row in face_df.iterrows()])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5af8b2fd64e77f8b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fix a seed for reproducibility\n",
    "seed_value = 42 \n",
    "# Numpy RNG\n",
    "np.random.seed(seed_value)\n",
    "# PyTorch RNGs\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed(seed_value)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0473360db5eeaa1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        \"\"\"\n",
    "        Custom dataset for the autoencoder\n",
    "        Args:\n",
    "            data (numpy.ndarray): A matrix containing your data.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        sample = Image.fromarray(sample)\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ede500360125c6ae"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Autoencoder\n",
    "For autoencoder, we will try two different networks and compare their differences: a fully connected network and a CNN network.\n",
    "\n",
    "### Autoencoder Choice 1: Fully Connected Network\n",
    "1. Given the small dataset size, we will start with a relatively simple model to avoid overfitting. First layer: 12288 (input) to 1024; Second layer: 1024 to 256; Third layer: 256 to 64; Fourth layer: 64 to latent space dimension. The latent space dimension should be large enough to capture relevant features but small enough to enforce meaningful compression. We will experiment about the latent space dimension in grid search later.\n",
    "2. The decoder reconstructs the image by doing the reverse operations. \n",
    "3. Since we want to reverse it back to an image, so we want the output to be bounded. Therefore, we add a Sigmoid at the end of the decoder to limit the value between 0 and 1."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21b293fd03fe9df1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class EmojiAutoencoderLinear(nn.Module):\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        super(EmojiAutoencoderLinear, self).__init__()\n",
    "        self.params = params\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(64 * 64 * 3, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, params['bottleneck_dim'])\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(params['bottleneck_dim'], 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 64 * 64 * 3),\n",
    "            nn.Sigmoid()  # Use sigmoid to ensure output values are between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        bottleneck = self.encoder(x)\n",
    "        x = self.decoder(bottleneck)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d61526453dccf2f7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Autoencoder Choice 2: CNN Network\n",
    "Compared with simply fully connected layers, convolutional layers are more effective at capturing spatial hierarchies in image data.\n",
    "1. The stride of 2 and padding of 1 in the convolutional layers reduce the spatial dimensions of the output by half each time (e.g., from 64x64 to 32x32, then to 16x16, and so on). This down-sampling is part of what helps the network to compress the input data into a more manageable set of features.\n",
    "2. ReLU introduces non-linearity into the network, allowing the network to learn more complex patterns. It is favored in CNNs due to its computational efficiency and because it helps mitigate the vanishing gradient problem.\n",
    "3. Since we want to reverse it back to an image, so we want the output to be bounded. Therefore, we add a Sigmoid at the end of the decoder to limit the value between 0 and 1."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "283c72d9dd51c97a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class EmojiAutoencoderCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        super(EmojiAutoencoderCNN, self).__init__()\n",
    "        self.params = params\n",
    "        if self.params['batch_normalization'] is True:\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),  # Output: 16 x 32 x 32\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),  # Output: 32 x 16 x 16\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # Output: 64 x 8 x 8\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),  # Output: 128 x 4 x 4\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output: 8 x 8 x 64\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output: 16 x 16 x 32\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output: 32 x 32 x 16\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(16, 3, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output: 64 x 64 x 3\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "        else:\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),  # Output: 16 x 32 x 32\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),  # Output: 32 x 16 x 16\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # Output: 64 x 8 x 8\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),  # Output: 128 x 4 x 4\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output: 8 x 8 x 64\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output: 16 x 16 x 32\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output: 32 x 32 x 16\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(16, 3, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output: 64 x 64 x 3\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        bottleneck = self.encoder(x)\n",
    "        x = self.decoder(bottleneck)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f456f952190b824"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we will construct our autoencoder and start training.\n",
    "\n",
    "Since we have small dataset, we adopt regularization techniques to prevent overfitting: L2 weight decay and batch normalization(for CNN)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efc2fde6a017d8d6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class EmojiAutoencoder(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(EmojiAutoencoder, self).__init__()\n",
    "        self.params = params\n",
    "        if self.params['auto_encoder'] == 'linear':\n",
    "            self.auto_encoder = EmojiAutoencoderLinear(self.params)\n",
    "        elif self.params['auto_encoder'] == 'CNN':\n",
    "            self.auto_encoder = EmojiAutoencoderCNN(self.params)\n",
    "        else:\n",
    "            self.auto_encoder = EmojiAutoencoderCNN(self.params)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=self.params['learning_rate'], weight_decay=self.params['weight_decay'])\n",
    "        self.train_loader, self.valid_loader, self.test_loader = load_data(self.params)\n",
    "    \n",
    "    def forward(self, img):\n",
    "        return self.auto_encoder.forward(img)\n",
    "\n",
    "    def train(self):\n",
    "        train_losses = []\n",
    "        validation_losses = []\n",
    "\n",
    "        for epoch in range(self.params['epoch']):\n",
    "            train_loss = 0\n",
    "            for data in self.train_loader:\n",
    "                img = data\n",
    "                if self.params['auto_encoder'] == 'linear':\n",
    "                    img = img.view(img.size(0), -1)  # Flatten the images\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.forward(img)\n",
    "                loss = self.criterion(outputs, img)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            train_loss /= len(self.train_loader)\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "            # Validation\n",
    "            validation_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for data in self.valid_loader:\n",
    "                    img = data\n",
    "                    if self.params['auto_encoder'] == 'linear':\n",
    "                        img = img.view(img.size(0), -1)\n",
    "                    outputs = self.forward(img)\n",
    "                    loss = self.criterion(outputs, img)\n",
    "                    validation_loss += loss.item()\n",
    "            validation_loss /= len(self.valid_loader)\n",
    "            validation_losses.append(validation_loss)\n",
    "            print(f'Epoch {epoch + 1}/{self.params[\"epoch\"]}, Train Loss: {train_loss:.4f}, Validation Loss: {validation_loss:.4f}')\n",
    "\n",
    "        plt.figure(figsize=[8, 6])\n",
    "        plt.plot(train_losses, label='Training Loss')\n",
    "        plt.plot(validation_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epochs', fontsize=14)\n",
    "        plt.ylabel('Loss', fontsize=14)\n",
    "        plt.title('Training and Validation Loss Curves', fontsize=16)\n",
    "        plt.legend()\n",
    "        # fig_name = f'{self.params[\"auto_encoder\"]}_learning_curve_{self.params[\"batch_size\"]}_{self.params[\"epoch\"]}_{self.params[\"learning_rate\"]}_{self.params[\"weight_decay\"]}_{self.params[\"batch_normalization\"]}_{self.params[\"bottleneck_dim\"]}.jpg'\n",
    "        # plt.savefig(fig_name, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def test(self):\n",
    "        total_mse_error = 0.0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for data in self.test_loader:\n",
    "                img = data\n",
    "                if self.params['auto_encoder'] == 'linear':\n",
    "                    img = img.view(img.size(0), -1)\n",
    "                reconstructed = self.auto_encoder.forward(img)\n",
    "                mse_error = self.criterion(reconstructed, img)\n",
    "                total_mse_error += mse_error.item() * img.size(0)  # Multiply by batch size to accumulate error correctly\n",
    "                total_samples += img.size(0)\n",
    "            average_mse_error = total_mse_error / total_samples\n",
    "            print(f'Test Average MSE Error: {average_mse_error:.4f}')\n",
    "            self.visual_inspection()\n",
    "            return average_mse_error\n",
    "\n",
    "    def visual_inspection(self):\n",
    "        # Visual inspect the difference between original images and reconstructed images\n",
    "        dataiter = iter(self.test_loader)  \n",
    "        images = next(dataiter)\n",
    "        if self.params['auto_encoder'] == 'linear':\n",
    "            images = images.view(images.size(0), -1)\n",
    "        reconstructed = self.auto_encoder.forward(images)\n",
    "        images = images.view(-1, 3, 64, 64)  # Reshape original images to proper shape\n",
    "        reconstructed = reconstructed.view(-1, 3, 64, 64)\n",
    "        images = images.numpy()\n",
    "        reconstructed = reconstructed.detach().numpy()\n",
    "\n",
    "        fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(10, 4))\n",
    "        for i in range(5):\n",
    "            ax = axes[0, i]\n",
    "            image_clipped = np.clip(np.transpose(images[i], (1, 2, 0)), 0, 1)\n",
    "            ax.imshow(image_clipped)  # Convert from (C, H, W) to (H, W, C)\n",
    "            ax.set_title('Original')\n",
    "            ax.axis('off')\n",
    "            ax = axes[1, i]\n",
    "            reconstructed_clipped = np.clip(np.transpose(reconstructed[i], (1, 2, 0)), 0, 1)\n",
    "            ax.imshow(reconstructed_clipped)\n",
    "            ax.set_title('Reconstructed')\n",
    "            ax.axis('off')\n",
    "        # fig_name = f'{self.params[\"auto_encoder\"]}_reconstructed_{self.params[\"batch_size\"]}_{self.params[\"epoch\"]}_{self.params[\"learning_rate\"]}_{self.params[\"weight_decay\"]}_{self.params[\"batch_normalization\"]}_{self.params[\"bottleneck_dim\"]}.jpg'\n",
    "        # plt.savefig(fig_name, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "    def encode(self, img):\n",
    "        return self.auto_encoder.encoder(img)\n",
    "    \n",
    "    def decode(self, encoded_img):\n",
    "        return self.auto_encoder.decoder(encoded_img)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "553f454325b4e183"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_data(params):\n",
    "    # Read data\n",
    "    emojis = pd.read_parquet(DATA_PATH, engine='pyarrow')\n",
    "    face_df = emojis[emojis['text'].apply(lambda x: 'face' in x.split() and 'clock' not in x.split())]\n",
    "    face_imgs = np.array([np.array(Image.open(io.BytesIO(row['image']['bytes']))) for index, row in face_df.iterrows()])\n",
    "\n",
    "    # Split dataset\n",
    "    total_length = len(face_df)\n",
    "    train_length = int(total_length * 0.6)\n",
    "    valid_length = int(total_length * 0.2)\n",
    "    test_length = total_length - train_length - valid_length\n",
    "    train_dataset, valid_dataset, test_dataset = random_split(face_imgs, [train_length, valid_length, test_length])\n",
    "\n",
    "    # Augment training data\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.RandomAffine(\n",
    "            degrees=15,  # Rotation: A small degree, since large rotations might make emojis unrecognizable.\n",
    "            translate=(0.1, 0.1),  # Translation: Shift the image by 10% of its height/width in any direction.\n",
    "            scale=(0.9, 1.1),  # Scale: Slightly zoom in or out by 10%.\n",
    "            shear=5  # Shear: Apply a small shearing of 5 degrees.\n",
    "        ),\n",
    "        transforms.RandomHorizontalFlip(),  # Horizontally flip the image with a given probability.\n",
    "        transforms.RandomVerticalFlip(p=0.5),  # Vertically flip the image with a given probability.\n",
    "        transforms.RandomRotation(20),  # Rotate the image by angle.\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # Randomly change the brightness, contrast, and saturation of an image.\n",
    "        transforms.ToTensor(),  # Convert a PIL Image or numpy.ndarray to tensor.\n",
    "        # transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize a tensor image with mean and standard deviation.\n",
    "    ])\n",
    "\n",
    "    # Augment validation data\n",
    "    validation_transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1),  # Mild augmentations\n",
    "        transforms.RandomHorizontalFlip(),  # Horizontally flip the image with a given probability.\n",
    "        transforms.RandomVerticalFlip(p=0.5),  # Vertically flip the image with a given probability.\n",
    "        transforms.RandomRotation(20),  # Rotate the image by angle.\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize a tensor image with mean and standard deviation.\n",
    "    ])\n",
    "\n",
    "    # Augment test data\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        # transforms.ColorJitter(brightness=0.1, contrast=0.1),  # Mild augmentations\n",
    "        # transforms.RandomHorizontalFlip(),  # Horizontally flip the image with a given probability.\n",
    "        # transforms.RandomVerticalFlip(p=0.5),  # Vertically flip the image with a given probability.\n",
    "        # transforms.RandomRotation(20),  # Rotate the image by angle.\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize a tensor image with mean and standard deviation.\n",
    "    ])\n",
    "\n",
    "    training_dataset = CustomDataset(train_dataset, transform=train_transform)\n",
    "    validation_dataset = CustomDataset(valid_dataset, transform=validation_transform)\n",
    "    testing_dataset = CustomDataset(test_dataset, transform=test_transform)\n",
    "    train_loader = DataLoader(training_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "    valid_loader = DataLoader(validation_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "    test_loader = DataLoader(testing_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "    return train_loader, valid_loader, test_loader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb53725d9bbb264c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To find the best parameter combinations for the autoencoder, we define a grid search function."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bdb1c522c5cc00d6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def grid_search():\n",
    "    params = {\n",
    "        'epoch': None,\n",
    "        'batch_size': None,\n",
    "        'learning_rate': None,\n",
    "        'bottleneck_dim': None,\n",
    "        'weight_decay': None,\n",
    "        'auto_encoder': None,\n",
    "        'batch_normalization': None\n",
    "    }\n",
    "    epoch = [100, 200]\n",
    "    batch_size = [16, 32, 64]\n",
    "    learning_rate = [0.0001, 0.001, 0.01, 0.1]\n",
    "    bottleneck_dim = [16, 32, 64]\n",
    "    weight_decay = [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    batch_normalization = [True, False]\n",
    "    CNN_param_combinations = list(itertools.product(epoch, batch_size, learning_rate, weight_decay, batch_normalization, ['CNN']))\n",
    "    linear_param_combinations = list(itertools.product(epoch, batch_size, learning_rate, weight_decay, bottleneck_dim, ['linear']))\n",
    "    # CNN network\n",
    "    grid_search_data = []\n",
    "    for combo in CNN_param_combinations:\n",
    "        keys = ['epoch', 'batch_size', 'learning_rate', 'weight_decay', 'batch_normalization', 'auto_encoder']\n",
    "        params_update = dict(zip(keys, combo))\n",
    "        params.update(params_update)\n",
    "        autoencoder = EmojiAutoencoder(params)\n",
    "        autoencoder.train()\n",
    "        params['error_rate'] = autoencoder.test()\n",
    "        print(params)\n",
    "        grid_search_data.append(params.copy())\n",
    "    # linear network\n",
    "    for combo in linear_param_combinations:\n",
    "        keys = ['epoch', 'batch_size', 'learning_rate', 'weight_decay', 'bottleneck_dim', 'auto_encoder']\n",
    "        params_update = dict(zip(keys, combo))\n",
    "        params.update(params_update)\n",
    "        autoencoder = EmojiAutoencoder(params)\n",
    "        autoencoder.train()\n",
    "        params['error_rate'] = autoencoder.test()\n",
    "        print(params)\n",
    "        grid_search_data.append(params.copy())\n",
    "    df = pd.DataFrame(grid_search_data)\n",
    "    # df.to_csv('CNN_params_grid_search_final.csv', index=True)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6dba9344deb4bd2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since it takes too long to run grid search, we have prepared the results here: https://github.com/msai437-group3/hw2/blob/mikky/autoencoder_params_grid_search.csv"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "811e4df1471fa87a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result_df = pd.read_csv('autoencoder_params_grid_search.csv')\n",
    "# result_df = grid_search()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20817efb3a8eefc9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Observations\n",
    "\n",
    "From the grid search results, we have below observations:\n",
    "\n",
    "1. The best autoencoder achieves an MSE error rate of 0.015.\n",
    "2. CNN outperforms fully connected network. The top 50 autoencoders all use CNN. \n",
    "3. Hyper parameters:\n",
    "- Batch size: Smaller batch size brings better performance. Smaller batch sizes introduce noise into the training process, which can have a regularizing effect, leading to better generalization. This can be particularly beneficial for small datasets, where overfitting is a significant concern. Smaller batch sizes can also lead to models that generalize better to unseen data. \n",
    "- Learning rate: A larger step size can propel the parameters out of these local minima, leading to better solutions.\n",
    "- Regularization:\n",
    "    - A smaller weight decay is more suitable here. It's not uncommon to see values like 1e-4 or 1e-5 used in conjunction with Adam optimizer, as these values tend to be small enough not to distort the model's learned weights too harshly while still helping to prevent overfitting.\n",
    "    - Batch normalization helps to decrease error rate. First, batch normalization helps to reduce the internal covariate shift, which is the change in the distribution of network activations due to the change in network parameters during training. By normalizing the inputs across mini-batches, batch normalization makes the training process faster and more stable. Second, because batch normalization stabilizes the learning process, it allows for the use of higher learning rates, which can make the training faster without the risk of divergence. Third, batch normalization adds a slight noise to the activations within each batch. This can be thought of as a form of regularization, helping to prevent overfitting."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29b53665b3884080"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result_df.sort_values(\"error_rate\", ascending=True)[:10]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5bf2d42503a0211b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "average_values = result_df.groupby('auto_encoder')['error_rate'].mean()\n",
    "average_values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3705310057ffea54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the type of auto encoders for the top 50 performer\n",
    "result_df.sort_values(\"error_rate\", ascending=True)[:50].groupby('auto_encoder').size()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c0527ea40ddf769"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also take a look at the best performer with fully connected layer. MSE loss of 0.077 is a terrible rate for reconstruction."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90ad0789e7535be3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result_df[result_df['auto_encoder'] == 'linear'].sort_values(\"error_rate\", ascending=True)[:10]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "836c5ad024dcafeb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cnn_df = result_df[result_df['auto_encoder'] == 'CNN']\n",
    "cnn_df = cnn_df.loc[:, cnn_df.columns != 'auto_encoder']\n",
    "correlation = cnn_df.corrwith(cnn_df['error_rate'])\n",
    "correlation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b59b2dc46a6d20fb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "linear_df = result_df[result_df['auto_encoder'] == 'linear']\n",
    "linear_df = linear_df.loc[:, linear_df.columns != 'auto_encoder']\n",
    "correlation = linear_df.corrwith(linear_df['error_rate'])\n",
    "correlation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7e50c8ac86dd358"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Results \n",
    "Let's take a look at the best parameter combination:\n",
    "1. training and validation loss learning curves\n",
    "2. final average MSE error on the test set\n",
    "3. reconstructed images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e987524eb07eee0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params = {\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 0.01,\n",
    "    'epoch': 200,\n",
    "    'auto_encoder': 'CNN',\n",
    "    'weight_decay': 1e-5,\n",
    "    'batch_normalization': False\n",
    "}\n",
    "autoencoder = EmojiAutoencoder(params)\n",
    "autoencoder.train()\n",
    "autoencoder.test()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a25c3dd09ef4220d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question2\n",
    "\n",
    "(5.0points) Separate your dataset into two or more classes using Emoji descriptions and assign labels. Repeat Step 1 adding image classification as an auxiliary task to MSE with a lambda of your choosing. You can choose any classification technique.\n",
    "a. describe how you separated your dataset into classes,\n",
    "b. describe your classification technique and hyperparameters,\n",
    "c. plot learning curves for training and validation loss for MSE and classification accuracy,\n",
    "d. discuss how incorporating classification as an auxiliary tasks impacts the performance of your autoencoder,\n",
    "e. speculate why performance changed and recommend (but do not implement) an experiment to confirm or reject your speculation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "660fb4f01cb7c8d2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classes\n",
    "\n",
    "Based on face attributes, we manually split the face dataset into four classes:\n",
    "human face, person, animal, other"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8fa487c5c47ea649"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "    'animal': ['smiling cat face with open mouth',\n",
    "               'grinning cat face with smiling eyes',\n",
    "               'cat face with tears of joy',\n",
    "               'smiling cat face with heart shaped eyes',\n",
    "               'cat face with wry smile',\n",
    "               'kissing cat face with closed eyes',\n",
    "               'weary cat face',\n",
    "               'crying cat face',\n",
    "               'pouting cat face',\n",
    "               'monkey face',\n",
    "               'dog face',\n",
    "               'wolf face',\n",
    "               'fox face',\n",
    "               'cat face',\n",
    "               'lion face',\n",
    "               'tiger face',\n",
    "               'horse face',\n",
    "               'unicorn face',\n",
    "               'zebra face',\n",
    "               'cow face',\n",
    "               'pig face',\n",
    "               'giraffe face',\n",
    "               'mouse face',\n",
    "               'hamster face',\n",
    "               'rabbit face',\n",
    "               'bear face',\n",
    "               'panda face',\n",
    "               'frog face',\n",
    "               'dragon face'],\n",
    "    'human face': ['grinning face',\n",
    "               'smiling face with open mouth',\n",
    "               'winking face',\n",
    "               'smiling face with smiling eyes',\n",
    "               'smiling face with halo',\n",
    "               'smiling face with smiling eyes and three hearts',\n",
    "               'smiling face with heart shaped eyes',\n",
    "               'grinning face with star eyes',\n",
    "               'face throwing a kiss',\n",
    "               'kissing face',\n",
    "               'white smiling face',\n",
    "               'kissing face with closed eyes',\n",
    "               'smiling face with open mouth and smiling eyes',\n",
    "               'kissing face with smiling eyes',\n",
    "               'face savouring delicious food',\n",
    "               'face with stuck out tongue',\n",
    "               'face with stuck out tongue and winking eye',\n",
    "               'grinning face with one large and one small eye',\n",
    "               'face with stuck out tongue and tightly closed eyes',\n",
    "               'money mouth face',\n",
    "               'hugging face',\n",
    "               'smiling face with smiling eyes and hand covering mouth',\n",
    "               'face with finger covering closed lips',\n",
    "               'grinning face with smiling eyes',\n",
    "               'thinking face',\n",
    "               'zipper mouth face',\n",
    "               'face with one eyebrow raised',\n",
    "               'neutral face',\n",
    "               'expressionless face',\n",
    "               'face without mouth',\n",
    "               'smirking face',\n",
    "               'unamused face',\n",
    "               'face with rolling eyes',\n",
    "               'grimacing face',\n",
    "               'smiling face with open mouth and tightly closed eyes',\n",
    "               'lying face',\n",
    "               'relieved face',\n",
    "               'pensive face',\n",
    "               'sleepy face',\n",
    "               'drooling face',\n",
    "               'sleeping face',\n",
    "               'face with medical mask',\n",
    "               'face with thermometer',\n",
    "               'face with head bandage',\n",
    "               'nauseated face',\n",
    "               'smiling face with open mouth and cold sweat',\n",
    "               'face with open mouth vomiting',\n",
    "               'sneezing face',\n",
    "               'overheated face',\n",
    "               'freezing face',\n",
    "               'face with uneven eyes and wavy mouth',\n",
    "               'dizzy face',\n",
    "               'shocked face with exploding head',\n",
    "               'face with cowboy hat',\n",
    "               'face with party horn and party hat',\n",
    "               'smiling face with sunglasses',\n",
    "               'nerd face',\n",
    "               'face with monocle',\n",
    "               'confused face',\n",
    "               'worried face',\n",
    "               'slightly frowning face',\n",
    "               'white frowning face',\n",
    "               'face with open mouth',\n",
    "               'hushed face',\n",
    "               'astonished face',\n",
    "               'flushed face',\n",
    "               'face with tears of joy',\n",
    "               'face with pleading eyes',\n",
    "               'frowning face with open mouth',\n",
    "               'anguished face',\n",
    "               'fearful face',\n",
    "               'face with open mouth and cold sweat',\n",
    "               'disappointed but relieved face',\n",
    "               'crying face',\n",
    "               'loudly crying face',\n",
    "               'face screaming in fear',\n",
    "               'confounded face',\n",
    "               'slightly smiling face',\n",
    "               'persevering face',\n",
    "               'disappointed face',\n",
    "               'face with cold sweat',\n",
    "               'weary face',\n",
    "               'tired face',\n",
    "               'face with look of triumph',\n",
    "               'pouting face',\n",
    "               'angry face',\n",
    "               'serious face with symbols covering mouth',\n",
    "               'smiling face with horns',\n",
    "               'upside down face'],\n",
    "    'other': ['robot face',\n",
    "               'new moon with face',\n",
    "               'first quarter moon with face',\n",
    "               'last quarter moon with face',\n",
    "               'full moon with face',\n",
    "               'sun with face',\n",
    "               'wind blowing face',\n",
    "               'clown face'],\n",
    "    'person': ['face massage',\n",
    "               'face massage',\n",
    "               'face massage',\n",
    "               'face massage',\n",
    "               'face massage',\n",
    "               'face massage',\n",
    "               'man getting face massage',\n",
    "               'man getting face massage type 1 2',\n",
    "               'man getting face massage type 3',\n",
    "               'man getting face massage type 4',\n",
    "               'man getting face massage type 5',\n",
    "               'man getting face massage type 6',\n",
    "               'woman getting face massage',\n",
    "               'woman getting face massage type 1 2',\n",
    "               'woman getting face massage type 3',\n",
    "               'woman getting face massage type 4',\n",
    "               'woman getting face massage type 5',\n",
    "               'woman getting face massage type 6',\n",
    "               'person with pouting face',\n",
    "               'person with pouting face',\n",
    "               'person with pouting face',\n",
    "               'person with pouting face',\n",
    "               'person with pouting face',\n",
    "               'person with pouting face',\n",
    "               'face with no good gesture',\n",
    "               'face with no good gesture',\n",
    "               'face with no good gesture',\n",
    "               'face with no good gesture',\n",
    "               'face with no good gesture',\n",
    "               'face with no good gesture',\n",
    "               'face with ok gesture',\n",
    "               'face with ok gesture',\n",
    "               'face with ok gesture',\n",
    "               'face with ok gesture',\n",
    "               'face with ok gesture',\n",
    "               'face with ok gesture',\n",
    "               'face palm',\n",
    "               'face palm',\n",
    "               'face palm',\n",
    "               'face palm',\n",
    "               'face palm',\n",
    "               'face palm']}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f2c27d6478759d0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Face class 1. human face"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47a8ff2d81fc04d6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filtered_df = face_df[face_df['text'].isin(label_dict['human face'])]\n",
    "show_emojis(filtered_df, fig_size=(30,40), subplot=(12,16), num=len(face_df), maximum_word=3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbf4eb7ed9dd6dc1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Face class 2. person"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95d90f897e1e9b02"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filtered_df = face_df[face_df['text'].isin(label_dict['person'])]\n",
    "show_emojis(filtered_df, fig_size=(30,40), subplot=(12,16), num=len(face_df), maximum_word=3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f744a592d8963ac"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Face class 3. animal"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c4b6294d4cf8231"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filtered_df = face_df[face_df['text'].isin(label_dict['animal'])]\n",
    "show_emojis(filtered_df, fig_size=(30,40), subplot=(12,16), num=len(face_df), maximum_word=3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b5098a4a13fec77"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Face class 4. other"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f888682d867dcf3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filtered_df = face_df[face_df['text'].isin(label_dict['other'])]\n",
    "show_emojis(filtered_df, fig_size=(30,40), subplot=(12,16), num=len(face_df), maximum_word=3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e909d3aac981567c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CustomDatasetClassification(Dataset):\n",
    "    # new custom dataset for classification problem\n",
    "    def __init__(self, features, labels, transform=None):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        feature = Image.fromarray(feature)\n",
    "        if self.transform:\n",
    "            feature = self.transform(feature)\n",
    "        return feature, self.labels[idx]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e9f03af338981dc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_data_classification(params):\n",
    "    # Read data\n",
    "    emojis = pd.read_parquet(DATA_PATH, engine='pyarrow')\n",
    "    face_df = emojis[emojis['text'].apply(lambda x: 'face' in x.split() and 'clock' not in x.split())].copy()\n",
    "    text_label_mapping = {item: label for label, text_list in label_dict.items() for item in text_list}\n",
    "    face_df['label'] = face_df['text'].replace(text_label_mapping)\n",
    "    face_features = np.array([Image.open(io.BytesIO(item.get('bytes'))) for item in face_df['image'].values])\n",
    "    face_labels = np.array(face_df['label'].values)\n",
    "    unique_labels = set(face_labels) \n",
    "    label_to_index = {label: index for index, label in enumerate(unique_labels)}\n",
    "    label_indices = np.array([label_to_index[label] for label in face_labels])\n",
    "    full_dataset = CustomDatasetClassification(face_features, label_indices) \n",
    "\n",
    "    # Split dataset\n",
    "    total_length = len(full_dataset)\n",
    "    train_length = int(total_length * 0.6)\n",
    "    valid_length = int(total_length * 0.2)\n",
    "    test_length = total_length - train_length - valid_length\n",
    "    train_data, valid_data, test_data = random_split(full_dataset, [train_length, valid_length, test_length])\n",
    "\n",
    "    # Augment training data\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.RandomAffine(\n",
    "            degrees=15,  # Rotation: A small degree, since large rotations might make emojis unrecognizable.\n",
    "            translate=(0.1, 0.1),  # Translation: Shift the image by 10% of its height/width in any direction.\n",
    "            scale=(0.9, 1.1),  # Scale: Slightly zoom in or out by 10%.\n",
    "            shear=5  # Shear: Apply a small shearing of 5 degrees.\n",
    "        ),\n",
    "        transforms.RandomHorizontalFlip(),  # Horizontally flip the image with a given probability.\n",
    "        transforms.RandomVerticalFlip(p=0.5),  # Vertically flip the image with a given probability.\n",
    "        transforms.RandomRotation(20),  # Rotate the image by angle.\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        # Randomly change the brightness, contrast, and saturation of an image.\n",
    "        transforms.ToTensor(),  # Convert a PIL Image or numpy.ndarray to tensor.\n",
    "        # transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize a tensor image with mean and standard deviation.\n",
    "    ])\n",
    "\n",
    "    # Augment validation data: Define light transformations for augmentation\n",
    "    valid_transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1),  # Mild augmentations\n",
    "        transforms.RandomHorizontalFlip(),  # Horizontally flip the image with a given probability.\n",
    "        transforms.RandomVerticalFlip(p=0.5),  # Vertically flip the image with a given probability.\n",
    "        transforms.RandomRotation(20),  # Rotate the image by angle.\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize a tensor image with mean and standard deviation.\n",
    "    ])\n",
    "\n",
    "    # Augment test data: Define light transformations for augmentation\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        # transforms.ColorJitter(brightness=0.1, contrast=0.1),  # Mild augmentations\n",
    "        # transforms.RandomHorizontalFlip(),  # Horizontally flip the image with a given probability.\n",
    "        # transforms.RandomVerticalFlip(p=0.5),  # Vertically flip the image with a given probability.\n",
    "        # transforms.RandomRotation(20),  # Rotate the image by angle.\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize a tensor image with mean and standard deviation.\n",
    "    ])\n",
    "\n",
    "    train_dataset = CustomDatasetClassification(train_data.dataset.features[train_data.indices],\n",
    "                                  train_data.dataset.labels[train_data.indices], train_transform)\n",
    "    valid_dataset = CustomDatasetClassification(valid_data.dataset.features[valid_data.indices],\n",
    "                                  valid_data.dataset.labels[valid_data.indices], valid_transform)\n",
    "    test_dataset = CustomDatasetClassification(test_data.dataset.features[test_data.indices],\n",
    "                                 test_data.dataset.labels[test_data.indices], test_transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "    return train_loader, valid_loader, test_loader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd4e7347615c7569"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Structure\n",
    "\n",
    "- Since CNN outperforms fully connected models observed in Question 1, we will use CNN as our auto encoder. \n",
    "- The encoder will be connected with several fully connected layers as classification network.\n",
    "- The final classification layer has four outputs with four class labels.\n",
    "- We will run grid search to decide how many layers should be included in this auxiliary classification task.\n",
    "- To prevent overfitting, we will further add dropout to the final fully connected layers."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7742f8762de8c73"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class EmojiAutoencoderWithClassifier(nn.Module):\n",
    "    def __init__(self, params, num_classes):\n",
    "        super(EmojiAutoencoderWithClassifier, self).__init__()\n",
    "        self.params = params\n",
    "        self.auto_encoder = EmojiAutoencoderCNN(self.params)\n",
    "\n",
    "        if self.params['classification_layer_num'] == 1:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(2048, num_classes)\n",
    "            )\n",
    "        elif self.params['classification_layer_num'] == 2:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(2048, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(self.params['dropout']),\n",
    "                nn.Linear(256, num_classes)\n",
    "            )\n",
    "        elif self.params['classification_layer_num'] == 3:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(2048, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(self.params['dropout']),\n",
    "                nn.Linear(256, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(self.params['dropout']),\n",
    "                nn.Linear(32, num_classes)\n",
    "            )\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=self.params['learning_rate'], weight_decay=self.params['weight_decay'])\n",
    "        self.train_loader, self.valid_loader, self.test_loader = load_data_classification(self.params)\n",
    "        self.mse_loss_function = nn.MSELoss()\n",
    "        self.cross_entropy_loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        bottleneck = self.auto_encoder.encoder(x)\n",
    "        decoded_features = self.auto_encoder.decoder(bottleneck)\n",
    "        class_logits = self.classifier(bottleneck.view(bottleneck.size(0), -1))\n",
    "        return decoded_features, class_logits\n",
    "\n",
    "    def train(self):\n",
    "        train_losses = []\n",
    "        train_accuracies = []\n",
    "        valid_losses = []\n",
    "        valid_accuracies = []\n",
    "\n",
    "        for epoch in range(self.params['epoch']):\n",
    "            train_loss = 0\n",
    "            correct_train = 0\n",
    "            total_train = 0\n",
    "            for data in self.train_loader:\n",
    "                imgs, labels = data\n",
    "                self.optimizer.zero_grad()\n",
    "                decoded_features, class_logits = self.forward(imgs)\n",
    "                # Calculate total loss\n",
    "                reconstruction_loss = self.mse_loss_function(decoded_features, imgs)\n",
    "                classification_loss = self.cross_entropy_loss_function(class_logits, labels)\n",
    "                loss = reconstruction_loss + self.params[\"classification_lambda\"] * classification_loss\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(class_logits.data, 1)\n",
    "                total_train += labels.size(0)\n",
    "                correct_train += (predicted == labels).sum().item()\n",
    "            train_loss /= len(self.train_loader)\n",
    "            train_losses.append(train_loss)\n",
    "            train_accuracies.append(100 * correct_train / total_train)\n",
    "\n",
    "            # Validation\n",
    "            valid_loss = 0\n",
    "            correct_valid = 0\n",
    "            total_valid = 0\n",
    "            with torch.no_grad():\n",
    "                for data in self.valid_loader:\n",
    "                    imgs, labels = data\n",
    "                    decoded_features, class_logits = self.forward(imgs)\n",
    "                    # Calculate total loss\n",
    "                    reconstruction_loss = self.mse_loss_function(decoded_features, imgs)\n",
    "                    classification_loss = self.cross_entropy_loss_function(class_logits, labels)\n",
    "                    loss = reconstruction_loss + self.params[\"classification_lambda\"] * classification_loss\n",
    "                    valid_loss += loss.item()\n",
    "                    # Calculate accuracy\n",
    "                    _, predicted = torch.max(class_logits.data, 1)\n",
    "                    total_valid += labels.size(0)\n",
    "                    correct_valid += (predicted == labels).sum().item()\n",
    "            valid_loss /= len(self.valid_loader)\n",
    "            valid_losses.append(valid_loss)\n",
    "            valid_accuracies.append(100 * correct_valid / total_valid)\n",
    "            print(f'Epoch {epoch + 1}/{self.params[\"epoch\"]}, '\n",
    "                  f'Train Loss: {train_losses[-1]:.4f}, '\n",
    "                  f'Train Accuracy: {train_accuracies[-1]:.2f}%, '\n",
    "                  f'Validation Loss: {valid_losses[-1]:.4f}, '\n",
    "                  f'Validation Accuracy: {valid_accuracies[-1]:.2f}%')\n",
    "\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label='Train MSE Loss')\n",
    "        plt.plot(valid_losses, label='Validation MSE Loss')\n",
    "        plt.title('Training and Validation MSE Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(train_accuracies, label='Train Accuracy')\n",
    "        plt.plot(valid_accuracies, label='Validation Accuracy')\n",
    "        plt.title('Training and Validation Classification Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        # fig_name = f'classification_learning_curve_{self.params[\"batch_size\"]}_{self.params[\"epoch\"]}_{self.params[\"learning_rate\"]}_{self.params[\"dropout\"]}_{self.params[\"classification_lambda\"]}_{self.params[\"classification_layer_num\"]}_{self.params[\"batch_normalization\"]}_{self.params[\"weight_decay\"]}.jpg'\n",
    "        # plt.savefig(fig_name, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def test(self):\n",
    "        total_mse_error = 0.0\n",
    "        total_samples = 0\n",
    "        total_test = 0\n",
    "        correct_test = 0\n",
    "        with torch.no_grad():\n",
    "            for data in self.test_loader:\n",
    "                imgs, labels = data\n",
    "                decoded_features, class_logits = self.forward(imgs)\n",
    "                reconstruction_loss = self.mse_loss_function(decoded_features, imgs)\n",
    "                total_mse_error += reconstruction_loss.item() * imgs.size(0)  # Multiply by batch size to accumulate error correctly\n",
    "                _, predicted = torch.max(class_logits.data, 1)\n",
    "                total_test += labels.size(0)\n",
    "                correct_test += (predicted == labels).sum().item()\n",
    "                total_samples += imgs.size(0)\n",
    "            # performance for autoencoder\n",
    "            average_mse_error = total_mse_error / total_samples\n",
    "            print(f'Test Average MSE Error: {average_mse_error:.4f}')\n",
    "            self.visual_inspection()\n",
    "            # performance for classifier\n",
    "            accuracy = 100 * correct_test / total_test\n",
    "            print(f'Test Average Accuracyr: {accuracy:.4f}')\n",
    "            return average_mse_error, accuracy\n",
    "\n",
    "    def visual_inspection(self):\n",
    "        # Visual inspect the difference between original images and reconstructed images\n",
    "        dataiter = iter(self.test_loader)\n",
    "        images, labels = next(dataiter)\n",
    "        reconstructed = self.auto_encoder.forward(images)\n",
    "        images = images.view(-1, 3, 64, 64)\n",
    "        reconstructed = reconstructed.view(-1, 3, 64, 64)\n",
    "        images = images.numpy()\n",
    "        reconstructed = reconstructed.detach().numpy()\n",
    "\n",
    "        fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(10, 4))\n",
    "        for i in range(5):\n",
    "            # Display original images\n",
    "            ax = axes[0, i]\n",
    "            image_clipped = np.clip(np.transpose(images[i], (1, 2, 0)), 0, 1)\n",
    "            ax.imshow(image_clipped)\n",
    "            ax.set_title('Original')\n",
    "            ax.axis('off')\n",
    "            # Display reconstructed images\n",
    "            ax = axes[1, i]\n",
    "            reconstructed_clipped = np.clip(np.transpose(reconstructed[i], (1, 2, 0)), 0, 1)\n",
    "            ax.imshow(reconstructed_clipped)\n",
    "            ax.set_title('Reconstructed')\n",
    "            ax.axis('off')\n",
    "        # fig_name = f'classification_reconstructed_{self.params[\"batch_size\"]}_{self.params[\"epoch\"]}_{self.params[\"learning_rate\"]}_{self.params[\"dropout\"]}_{self.params[\"classification_lambda\"]}_{self.params[\"classification_layer_num\"]}_{self.params[\"batch_normalization\"]}_{self.params[\"weight_decay\"]}.jpg'\n",
    "        # plt.savefig(fig_name, dpi=300, bbox_inches='tight')\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "adca8dd232b07ba7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def grid_search_classification():\n",
    "    params = {\n",
    "        'epoch': None,\n",
    "        'batch_size': None,\n",
    "        'learning_rate': None,\n",
    "        'dropout': None,\n",
    "        'classification_lambda': None,\n",
    "        'classification_layer_num': None\n",
    "    }\n",
    "    epoch = [100, 200]\n",
    "    batch_size = [16, 32]\n",
    "    learning_rate = [0.0001, 0.001, 0.01]\n",
    "    dropout = [0, 0.3, 0.5]\n",
    "    classification_lambda = [0.1, 0.5, 0.7]\n",
    "    classification_layer_num = [1, 2, 3]\n",
    "    batch_normalization = [True, False]\n",
    "    weight_decay = [1e-3, 1e-4, 1e-5]\n",
    "    param_combinations = list(itertools.product(epoch, batch_size, learning_rate, dropout, classification_lambda, classification_layer_num, batch_normalization, weight_decay))\n",
    "    grid_search_data = []\n",
    "    for combo in param_combinations:\n",
    "        keys = ['epoch', 'batch_size', 'learning_rate', 'dropout', 'classification_lambda', 'classification_layer_num', 'batch_normalization', 'weight_decay']\n",
    "        params_update = dict(zip(keys, combo))\n",
    "        params.update(params_update)\n",
    "        classifier = EmojiAutoencoderWithClassifier(params, 4)\n",
    "        classifier.train()\n",
    "        params['reconstruction_error_rate'], params['classification_accuracy'] = classifier.test()\n",
    "        print(params)\n",
    "        grid_search_data.append(params.copy())\n",
    "    df = pd.DataFrame(grid_search_data)\n",
    "    # df.to_csv('classification_params_grid_search.csv', index=True)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b080bd46ed4bbaf5"
  },
  {
   "cell_type": "raw",
   "source": [
    "Since it takes too long to run grid search, we have prepared the results here: https://github.com/msai437-group3/hw2/blob/mikky/classification_params_grid_search.csv"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a0c2f24300eb2eb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classification_result_df = pd.read_csv('classification_params_grid_search.csv')\n",
    "# classification_result_df = grid_search_classification()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d22358ab73d79b8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's take a look at the classification accuracy on the test data. The best performed model can reach 100%."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7729416b60d6c2f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classification_result_df.sort_values(['classification_accuracy', 'reconstruction_error_rate'], ascending=[False, True])[:20]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff5230d33dccae0c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "After adding auxiliary classification task, the best reconstruction error rate goes up from 0.015 to 0.0213, which means incorporating classification as an auxiliary tasks negatively impacts the performance of the autoencoder."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6dec8bcf73d1904"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classification_result_df.sort_values('reconstruction_error_rate', ascending=True)[:5]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52d5a4a39fa1f916"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "correlation_error_rate = classification_result_df.corrwith(classification_result_df['reconstruction_error_rate'])\n",
    "correlation_error_rate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f53d841f2cab1b43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "correlation_accuracy = classification_result_df.corrwith(classification_result_df['classification_accuracy'])\n",
    "correlation_accuracy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3e93a9f993122c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Observations\n",
    "1. Although the accuracy can be as high as 100%, after adding auxiliary classification task, the best reconstruction error rate goes up from 0.015 to 0.022.\n",
    "2. Classification accuracy is negatively correlated with reconstruction MSE loss.\n",
    "\n",
    "### Speculation \n",
    "The reason for the negative impact of auxiliary tasks on the autoencoder might be:\n",
    "- Classification accuracy is negatively correlated with reconstruction MSE loss, which means that the autoencoder and the classifier have competing objectives. The autoencoder tries to learn a compact representation that best reconstructs the input data, while the classifier tries to learn features that are most discriminative for the classification task. If these objectives don't align perfectly, the shared representation might compromise between reconstruction quality and classification accuracy, potentially leading to worse reconstructions.\n",
    "- Training Dynamics: The way the model is trained can also affect its performance. If the autoencoder was pre-trained before adding the classifier, and then both parts were fine-tuned together, the dynamics of learning might shift, impacting the quality of reconstruction. This can be due to changes in gradients and updates that now have to accommodate both tasks.\n",
    "- Overfitting to Classification: Adding a classifier introduces more parameters to the model, increasing its complexity. If the classifier overfits to the training data, it may lead the entire model, including the autoencoder part, to overfit as well. This means that while the classification accuracy might improve, the autoencoder could lose its ability to generalize well to unseen data, worsening reconstruction quality.\n",
    "\n",
    "\n",
    "### Experiment to confirm speculation\n",
    "#### Experiment 1\n",
    "Sequential Training: First, train the autoencoder alone until it reaches satisfactory reconstruction quality. Then, freeze the weights of the encoder (and possibly the decoder) and train only the classifier. \n",
    "#### Experiment 2\n",
    "More Regularization: Apply regularization techniques to prevent overfitting, especially to the classifier part. As we already implemented dropout, L2 regularization, data augmentation, we can further implement early stopping.\n",
    "#### Experiment 3\n",
    "Instead of using the same encoded representation for both reconstruction and classification, we can have separate branches after the initial layers of the encoder: one branch for reconstruction and one for classification. This allows each branch to learn features relevant to its specific objective without interfering too much with each other."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9e8a132c245c8e8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Results\n",
    "\n",
    "Let’s take a look at the best parameter combination:\n",
    "- training and validation loss learning curves\n",
    "- training and validation learning curves for classification accuracy\n",
    "- final average MSE error on the test set\n",
    "- reconstructed images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff30ad0be38f3007"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params = {\n",
    "    'epoch': 200,\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 0.001,\n",
    "    'dropout': 0.3,\n",
    "    'classification_lambda': 0.1,\n",
    "    'classification_layer_num': 1,\n",
    "    'weight_decay': 0.00001,\n",
    "    'batch_normalization': False\n",
    "}\n",
    "classifier = EmojiAutoencoderWithClassifier(params, 4)\n",
    "classifier.train()\n",
    "classifier.test()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e658a2d2edc981a3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 3\n",
    "(5.0 points) Select an attribute from the Emoji dataset (internal or external to your selected subset) to compose with any image from your selected subset. Use vector arithmetic on latent representations to generate a composite image that expresses the attribute. For example, I chose to add the glasses from “nerd face” to the “face with stuck out tongue\n",
    "a. specify which attribute you selected, the vector arithmetic applied and the resulting image(s) as displayed above,\n",
    "b. provide a qualitative evaluation of your composite image,and\n",
    "c. discuss ways to improve the quality of your generated image."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "232549474eaaeed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will pick the best autoencoder from question 1."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "feec4fd0f8bd352b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_latent_representation(autoencoder, emoji_text):\n",
    "    emoji_info = emojis[emojis['text'] == emoji_text].iloc[0]\n",
    "    img = Image.open(io.BytesIO(emoji_info['image']['bytes']))\n",
    "    transform = transforms.Compose([transforms.Resize((64, 64)),transforms.ToTensor()])\n",
    "    emoji_tensor = transform(img)\n",
    "    latent_representation = autoencoder.encode(emoji_tensor)\n",
    "    return latent_representation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b60d8eb75d2562a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_reconstructed_image(latent_representation):\n",
    "    reconstructed = autoencoder.decode(latent_representation).view(-1, 3, 64, 64).detach().numpy()[0]\n",
    "    reconstructed = np.transpose(reconstructed, (1, 2, 0))\n",
    "    return reconstructed"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32234221f1f5e07"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def show_reconstructed_emojis(emoji_list, fig_size=(25,25), subplot=(10,10), maximum_word=4):\n",
    "    plt.figure(figsize=fig_size)\n",
    "    for i, emoji_text in enumerate(emoji_list, start=1):\n",
    "        latent_representation = get_latent_representation(autoencoder, emoji_text)\n",
    "        reconstructed = get_reconstructed_image(latent_representation)\n",
    "        ax = plt.subplot(subplot[0], subplot[1], i)\n",
    "        ax.axis('off')\n",
    "        ax.imshow(reconstructed)\n",
    "        ax.text(0.5, -0.5, format_text(emoji_text, max_words=maximum_word), fontsize=12, ha='center', transform=ax.transAxes)\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.1, hspace=1.2)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8c3d806a0ff62ce"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's first try the example in the homework file."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86cef65ac2d3b1f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "latent_1 = get_latent_representation(autoencoder, \"nerd face\")\n",
    "latent_2 = get_latent_representation(autoencoder, \"smiling face with open mouth\")\n",
    "latent_3 = get_latent_representation(autoencoder, \"face with stuck out tongue\")\n",
    "latent_combined = latent_1 - latent_2 + latent_3\n",
    "image_combined = get_reconstructed_image(latent_combined)\n",
    "plt.imshow(image_combined)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43129e0b91a340b5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The effect is not good. It indicates that we can not simply do arithmetic between latent representations of faces, instead, if we intend to combine different features of a face, we need to extract that specific feature (e.g. grinning mouth) and add up the features to an average face. The average face can be a neutral face with most common featured mouth, eyes, eyebrows. Here, we will pick up \"slightly smiling face\" or \"face without mouth\"."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "799f5e1280de0294"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "average_face_list = ['slightly smiling face', 'face without mouth']\n",
    "show_reconstructed_emojis(average_face_list)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85555d58a3bf9043"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next,let's pick up some featured mouths and eyes."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36be4afa09cb1362"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "featured_mouth_list = ['grinning face', 'smiling face with open mouth', \n",
    "              'smiling face with open mouth and smiling eyes', 'face with stuck out tongue', 'face with stuck out tongue and tightly closed eyes', 'grinning face with smiling eyes', \n",
    "              'smiling face with open mouth and tightly closed eyes', 'sleepy face', 'face with medical mask',\n",
    "              'smiling face with open mouth and cold sweat', 'face with open mouth vomiting', 'face with tears of joy',\n",
    "              'weary face', 'tired face', 'face with look of triumph']\n",
    "show_reconstructed_emojis(featured_mouth_list)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8dd96f2fb1789f31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "featured_eye_list = ['grinning face with star eyes',\n",
    "            'face with stuck out tongue and winking eye', \n",
    "            'face with rolling eyes', 'dizzy face', 'smiling face with sunglasses',\n",
    "            'flushed face', 'face with pleading eyes']\n",
    "show_reconstructed_emojis(featured_eye_list)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b518359d70017f70"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's generate a grinning face with sunglasses."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4516abde3c2faad7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "average_face = get_latent_representation(autoencoder, \"slightly smiling face\")\n",
    "\n",
    "# get sunglasses feature\n",
    "latent_1 = get_latent_representation(autoencoder, \"smiling face with sunglasses\")\n",
    "latent_sunglasses = latent_1 - average_face\n",
    "\n",
    "# get grinning mouth\n",
    "latent_2 = get_latent_representation(autoencoder, \"grinning face\")\n",
    "latent_grinning_mouth = latent_2 - average_face\n",
    "\n",
    "# add features to an average face\n",
    "latent_combined = average_face + latent_sunglasses + latent_grinning_mouth\n",
    "image_combined = get_reconstructed_image(latent_combined)\n",
    "# plt.imshow(image_combined)\n",
    "\n",
    "# plot faces\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(3, 1))\n",
    "axes[0].imshow(get_reconstructed_image(latent_1))\n",
    "axes[1].imshow(get_reconstructed_image(latent_2))\n",
    "axes[2].imshow(get_reconstructed_image(latent_combined))\n",
    "axes[0].axis('off')\n",
    "axes[1].axis('off')\n",
    "axes[2].axis('off')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73e8d51ec02a19f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pretty good! Let's try the example in the homework again."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7f9cc6cc743dcea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "average_face = get_latent_representation(autoencoder, \"slightly smiling face\")\n",
    "\n",
    "# get glasses feature\n",
    "latent_1 = get_latent_representation(autoencoder, \"nerd face\")\n",
    "latent_glasses = latent_1 - average_face\n",
    "\n",
    "# get grinning mouth feature\n",
    "latent_2 = get_latent_representation(autoencoder, \"smiling face with open mouth\")\n",
    "latent_smiling_mouth = latent_2 - average_face\n",
    "\n",
    "# get stuck out tongue feature\n",
    "latent_3 = get_latent_representation(autoencoder, \"face with stuck out tongue\")\n",
    "latent_stuck_out_tongue = latent_3 - average_face\n",
    "\n",
    "# add features to an average face\n",
    "latent_combined = latent_1 - latent_smiling_mouth + latent_stuck_out_tongue\n",
    "image_combined = get_reconstructed_image(latent_combined)\n",
    "# plt.imshow(image_combined)\n",
    "\n",
    "# plot faces\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(4, 1))\n",
    "axes[0].imshow(get_reconstructed_image(latent_1))\n",
    "axes[1].imshow(get_reconstructed_image(latent_2))\n",
    "axes[2].imshow(get_reconstructed_image(latent_3))\n",
    "axes[3].imshow(get_reconstructed_image(latent_combined))\n",
    "axes[0].axis('off')\n",
    "axes[1].axis('off')\n",
    "axes[2].axis('off')\n",
    "axes[3].axis('off')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6290542ef7c0cf82"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Horrible!\n",
    "\n",
    "### Qualitative evaluation:\n",
    "1. Visual Quality: \n",
    "    - The clarity and resolution is not ideal, which is due to the reconstruction ability of the autoencoder.\n",
    "    - The model is poor at expressing colors. As we can see from \"face with stuck out tongue\", the tongue is supposed to be pink but turns out to be grey in the reconstructed image.\n",
    "2. Inconsistent integration effect. Some features, such as grinning mouth, smiling mouth and sunglasses, are easy to integrate, while some features, such as glasses, can have wierd contours and unnatural overlaps. The reasons might be that we have 6 emojis with grinning mouth and only 1 emoji with glasses, so the model can learn \"grinning mouth\" better than \"glasses\". Some attributes are not well-presented in their latent vectors, which leads to their poor performance in blending. \n",
    "\n",
    "### Ways to improve the quality of your generated image:\n",
    "1. Improve the autoencoder:\n",
    "    - Data: \n",
    "        - Increase data: Our dataset is still small, which is subject to over-fitting. We can enrich the data by introducing external emoji datasets to improve the generalization ability.\n",
    "        - Balanced Attributes: Ensure that the attributes are well-represented in the latent space. For example, if blending \"smiling\" and \"glasses\" into a face, both attributes should be distinct and significant in their latent vectors.\n",
    "    - Change the structure of the autoencoder by adding more CNN layers to capture the detailed features.\n",
    "    - Add more color-sensitive network layers to capture the color features of the emojis.\n",
    "    - Increase the size of the latent space to capture more details\n",
    "2. Interpolation and Smoothing: Instead of directly adding or subtracting feature vectors, consider using interpolation (such as spherical linear interpolation or SLERP) between vectors for smoother transitions.\n",
    "3. Average Attribute Vectors: If aiming to add common attributes (like \"smiling\"), consider averaging several vectors representing that attribute from different images to create a more general representation.\n",
    "####  Advanced\n",
    "4. Use of GANs: Consider using Generative Adversarial Networks (GANs) for attribute blending. GANs can produce more realistic and higher-quality images and can be trained specifically for tasks like face attribute modification.\n",
    "5. Semantic Disentanglement: Work towards disentangling the latent space semantically, ensuring that different dimensions control distinct and interpretable aspects of the generated images."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dfdc63772581703c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
